---
title: "6Nov23_Train&Test_MODIS_new"
author: "Baloch_Ali"
date: "2023-11-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library(rpart)
library(rpart.plot)

#Step 1: import the dataset
set.seed(678)
data = read.csv("C:/Users/april/Downloads/20230807_httpscwfis.cfs.nrcan.gc.cadownloadshotspots_MODIS only1.csv")
head(data)

#data = read.csv("C:/Users/april/OneDrive/Desktop/Github_Desktop/TMU_CIND820?20230807_httpscwfis.cfs.nrcan.gc.cadownloadshotspots_MODIS only1.csv")

tail(data)
dim(data)

#Decision tree R code Explanation 
#sample(1:nrow(data)): Generate a random list of index from 1 to 500 (i.e. the maximum number of rows). I notice the data is not shuffled. This is a big issue! When I split the data between a train set and test set, 'Fire_NoFire'  will not have proper observations, which means the algorithm will never see the features based on attributes. This error will lead to poor predictions.

shuffle_index <- sample(1:nrow(data))
head(shuffle_index)

#use this index to shuffle the titanic dataset.
data <- data[shuffle_index, ]
head(data)

str(data)

#Step 2) Clean & Convert Fire_Nofire integer values of 1 or 2 to High or Low respectively
summary(is.na(data))
data$Fire_Nofire <- ifelse(data$Fire_Nofire == 1, "High", "Low")

clean_data <- data[, -c(2, 3, 4, 9)]
# Display the new dataframe
head(clean_data)
str(clean_data)

#install.packages("ggplot2")
#install.packages("corrplot")

library(ggplot2)
library(corrplot)
numeric_data <- clean_data[, sapply(clean_data, is.numeric)]
correlation_matrix <- cor(numeric_data)
corrplot(correlation_matrix, method = "color")

#Step 3) Create train/test set
create_train_test <- function(clean_data, size = 0.8, train = TRUE) {
    n_row = nrow(clean_data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (clean_data[train_sample, ])
    } else {
        return (clean_data[-train_sample, ])
    }
}

data_train <- create_train_test(clean_data, 0.8, train = TRUE)
dim(data_train)

data_test <- create_train_test(clean_data, 0.8, train = FALSE)
dim(data_test)

#using the function prop.table() combined with table() to verify if the randomization process is correct.
prop.table(table(data_train$Fire_Nofire))
prop.table(table(data_test$Fire_Nofire))

#In both datasets, the amount of ""high" fires is the same, about 19 percent.

#Step 4) Build the model
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
fit <- rpart(Fire_Nofire ~ ., data = data_train, method = 'class')
rpart.plot(fit, extra = 106)

#Step 5) Make a prediction
predict_unseen <-predict(fit, data_test, type = 'class')

#table(data_test$Fire_Nofire, predict_unseen): Creating a table to count how many Fire_NoFire are classified as "survivors"High" or "Low" and compare to the correct decision tree classification in R
table_mat <- table(data_test$Fire_Nofire, predict_unseen)
table_mat

#The model correctly predicted 5 "high" fires but classified 14 incorrectly as "low". By analogy, the model misclassified 6 incorrectly.

#Step 6) Measure performance
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
print(paste('Accuracy for test', accuracy_Test))

#Recall (Sensitivity, True Positive Rate):Recall measures the ability of the classifier to identify all relevant instances in the positive class

true_positives <- table_mat[2, 2]  # Actual positive and predicted positive
false_negatives <- table_mat[2, 1]  # Actual positive but predicted negative
recall <- true_positives / (true_positives + false_negatives)
print(paste('Recall for test', recall))

#Precision measures the ability of the classifier to correctly identify positive instances out of all instances it predicted as positive. It is calculated as the ratio of true positives to the total predicted positives

true_positives <- table_mat[2, 2]  # Actual positive and predicted positive
false_positives <- table_mat[1, 2]  # Actual negative but predicted positive
precision <- true_positives / (true_positives + false_positives)
print(paste('Precision for test', precision))

#Step 7) Tune the parameters as follows: 
#Construct function to return accuracy
#Tune the maximum depth
#Tune the minimum number of sample a node must have before it can split
#Tune the minimum number of sample a leaf node must have

accuracy_tune <- function(fit) {predict_unseen <- predict(fit, data_test, type = 'class')
table_mat <- table(data_test$Fire_Nofire , predict_unseen)
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test}

control <- rpart.control(minsplit = 4, minbucket = round(5 / 3), maxdepth = 3, cp = 0)
tune_fit <- rpart(Fire_Nofire ~ ., data = data_train, method = 'class', control = control)
accuracy_tune(tune_fit)

control <- rpart.control(minsplit = 5, minbucket = round(5 / 3), maxdepth = 4, cp = 0)
tune_fit <- rpart(Fire_Nofire ~ ., data = data_train, method = 'class', control = control)
accuracy_tune(tune_fit)
# Looks good as I get a higher performance than the previous model!
rpart.plot(tune_fit, extra = 106)

```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
#plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
